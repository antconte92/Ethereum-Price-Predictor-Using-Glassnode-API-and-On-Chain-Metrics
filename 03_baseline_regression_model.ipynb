{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import warnings\n",
    "from functools import reduce\n",
    "\n",
    "from random import gauss as gs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "\n",
    "import itertools\n",
    "#from pmdarima import auto_arima\n",
    "\n",
    "#statsmodels\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Descriptions\n",
    "|Feature Name|Description|\n",
    "|------------|-----------|\n",
    "|active_addresses|The number of unique addresses that were active in the network either as a sender or receiver. Only addresses that were active in successful transactions are counted.|\n",
    "|circulating_supply|The total amount of all coins ever created/issued, i.e. the circulating supply.|\n",
    "|exchange_balance|The total amount of coins held on exchange addresses. Note that exchange metrics are based on our labeled data of exchange addresses that we constantly keep updating, as well as data science techniques and statistical information that changes over time. Therefore these metrics are mutable – the data is stable, but especially most recent data points are subject to slight fluctuations as time progresses.|\n",
    "|exchange_deposits|The total count of transfers to exchange addresses, i.e. the number of on-chain deposits to exchanges. Note that exchange metrics are based on our labeled data of exchange addresses that we constantly keep updating, as well as data science techniques and statistical information that changes over time. Therefore these metrics are mutable – the data is stable, but especially most recent data points are subject to slight fluctuations as time progresses.|\n",
    "|exchange_withdrawals|The total count of transfers from exchange addresses, i.e. the number of on-chain withdrawals from exchanges. Note that exchange metrics are based on our labeled data of exchange addresses that we constantly keep updating, as well as data science techniques and statistical information that changes over time. Therefore these metrics are mutable – the data is stable, but especially most recent data points are subject to slight fluctuations as time progresses.|\n",
    "|market_cap|The market capitalization (or network value) is defined as the product of the current supply by the current USD price.|\n",
    "|price|The asset's closing price in USD.|\n",
    "|sopr|The Spent Output Profit Ratio (SOPR) is computed by dividing the realized value (in USD) divided by the value at creation (USD) of a spent output. Or simply: price sold / price paid. This metric was created by Renato Shirakashi. For a detailed commentary see this post|\n",
    "|transaction_count|The total amount of transactions. Only successful transactions are counted.|\n",
    "|transfer_count|The total amount of transfers. One transaction can trigger one or more transfers. Only successful, non-zero transfers are counted.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/ethereum_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1827 entries, 2015-08-08 to 2020-08-11\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   active_addresses      1827 non-null   int64  \n",
      " 1   circulating_supply    1827 non-null   float64\n",
      " 2   exchange_balance      1827 non-null   float64\n",
      " 3   exchange_deposits     1827 non-null   int64  \n",
      " 4   exchange_withdrawals  1827 non-null   int64  \n",
      " 5   market_cap            1827 non-null   float64\n",
      " 6   price                 1827 non-null   float64\n",
      " 7   sopr                  1827 non-null   float64\n",
      " 8   transaction_count     1827 non-null   int64  \n",
      " 9   transfer_count        1827 non-null   int64  \n",
      "dtypes: float64(5), int64(5)\n",
      "memory usage: 157.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.67071902e-05 0.00000000e+00 0.00000000e+00 ... 4.88617387e-02\n",
      "  7.73758201e-04 2.73712687e-04]\n",
      " [0.00000000e+00 6.82289474e-04 2.07980583e-02 ... 2.13471515e-02\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.72813226e-04 1.38062211e-03 5.39909678e-02 ... 0.00000000e+00\n",
      "  5.36082474e-04 2.48910243e-04]\n",
      " ...\n",
      " [5.45313626e-01 9.99316549e-01 9.54212211e-01 ... 3.40106327e-01\n",
      "  8.81382568e-01 4.15623414e-01]\n",
      " [5.72517779e-01 9.99658065e-01 9.47039546e-01 ... 3.43872741e-01\n",
      "  9.25728210e-01 4.47526443e-01]\n",
      " [5.63989326e-01 1.00000000e+00 9.46644068e-01 ... 3.37968813e-01\n",
      "  9.02184817e-01 4.38561246e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler ()\n",
    "\n",
    "scaled_df = scaler.fit_transform(df)\n",
    "print(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ecce7f0467e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Drop the new 'age' column and our target column 'price' for the independent features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Set our dependent variable as price\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "# Drop the new 'age' column and our target column 'price' for the independent features\n",
    "X = scaled_df.drop('price', axis = 1)\n",
    "\n",
    "# Set our dependent variable as price\n",
    "y = scaled_df.price\n",
    "  \n",
    "# Split up our independent and dependent variables into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains our model on our baseline values\n",
    "lm_dummy_mean = DummyRegressor(strategy = 'mean').fit(X_train, y_train)\n",
    "\n",
    "# predicting test prices based on the X_test\n",
    "y_predict_dummy_mean = lm_dummy_mean.predict(X_test)\n",
    "\n",
    "# Find the error of our dummy model\n",
    "rmse_lr0 = mean_squared_error(y_test, y_predict_dummy_mean, squared=False)\n",
    "print(r2_score(y_test, y_predict_dummy_mean), rmse_lr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate our model and find the mean of the training scores\n",
    "scores_simple_1 = cross_validate(\n",
    "                    lr, X_train, y_train, cv=5, \n",
    "                    return_train_score=True\n",
    ")\n",
    "print(scores_simple_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_1_mean = np.mean(scores_simple_1['train_score'])\n",
    "print(simple_1_mean)\n",
    "simple_1_mean_test = np.mean(scores_simple_1['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_1_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the QQ Plot to understand the distribution of residuals\n",
    "residuals1 = (y_test - pred_lr1)\n",
    "sm.graphics.qqplot(residuals1, dist=stats.norm, line=\"45\", fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the model to our training dataset\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building - Basic Model (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fits the model to our training dataset\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict price with the trained model\n",
    "pred_lr2 = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_df = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
